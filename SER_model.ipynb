{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620},{"sourceId":639622,"sourceType":"datasetVersion","datasetId":316368},{"sourceId":653195,"sourceType":"datasetVersion","datasetId":325566},{"sourceId":671851,"sourceType":"datasetVersion","datasetId":338555}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n#IMPORT THE LIBRARIES\nimport pandas as pd\nimport numpy as np\n\nimport os\nimport sys\n\n# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\nimport librosa\nimport librosa.display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\n# to play the audio files\nimport IPython.display as ipd\nfrom IPython.display import Audio\nimport keras\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding\nfrom keras.layers import LSTM,BatchNormalization , GRU\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.layers import Input, Flatten, Dropout, Activation\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.optimizers import SGD\n\n\n\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nimport tensorflow as tf \nprint (\"Done\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T07:02:52.695621Z","iopub.execute_input":"2025-05-06T07:02:52.696226Z","iopub.status.idle":"2025-05-06T07:02:52.703147Z","shell.execute_reply.started":"2025-05-06T07:02:52.696204Z","shell.execute_reply":"2025-05-06T07:02:52.702396Z"}},"outputs":[{"name":"stdout","text":"Done\n","output_type":"stream"}],"execution_count":53},{"cell_type":"markdown","source":"Preparing Datasets","metadata":{}},{"cell_type":"code","source":"ravdess = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\nCrema = \"/kaggle/input/cremad/AudioWAV/\"\nTess = \"/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\nSavee = \"/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T07:02:52.704451Z","execution_failed":"2025-05-07T08:05:54.671Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Pre Processing**","metadata":{}},{"cell_type":"markdown","source":"ravdess","metadata":{}},{"cell_type":"code","source":"file_emotion = []\nfile_path = []\nravdess_directory_list = os.listdir(ravdess)\nfor i in ravdess_directory_list:\n    # as their are 24 different actors in our previous directory we need to extract files for each actor.\n    actor = os.listdir(ravdess + i)\n    for f in actor:\n        part = f.split('.')[0].split('-')\n    # third part in each file represents the emotion associated to that file.\n        file_emotion.append(int(part[2]))\n        file_path.append(ravdess + i + '/' + f)\n\n\n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nravdess_df = pd.concat([emotion_df, path_df], axis=1)\n# changing integers to actual emotions.\nravdess_df.Emotions.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust',\n                             8:'surprise'},\n                            inplace=True)\nprint(ravdess_df.head())\nprint(\"______________________________________________\")\nprint(ravdess_df.tail())\nprint(\"_______________________________________________\")\nprint(ravdess_df.Emotions.value_counts())\n\n\n    ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.671Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**CremaD**","metadata":{}},{"cell_type":"code","source":"crema_directory_list = os.listdir(Crema)\n\nfile_emotion = []\nfile_path = []\n\nfor file in crema_directory_list:\n    # storing file paths\n    file_path.append(Crema + file)\n    # storing file emotions\n    part=file.split('_')\n    if part[2] == 'SAD':\n        file_emotion.append('sad')\n    elif part[2] == 'ANG':\n        file_emotion.append('angry')\n    elif part[2] == 'DIS':\n        file_emotion.append('disgust')\n    elif part[2] == 'FEA':\n        file_emotion.append('fear')\n    elif part[2] == 'HAP':\n        file_emotion.append('happy')\n    elif part[2] == 'NEU':\n        file_emotion.append('neutral')\n    else:\n        file_emotion.append('Unknown')\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nCrema_df = pd.concat([emotion_df, path_df], axis=1)\nCrema_df.head()\nprint(Crema_df.Emotions.value_counts())\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.672Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**TESS**","metadata":{}},{"cell_type":"code","source":"tess_directory_list = os.listdir(Tess)\n\nfile_emotion = []\nfile_path = []\n\nfor dir in tess_directory_list:\n    directories = os.listdir(Tess + dir)\n    for file in directories:\n        part = file.split('.')[0]\n        part = part.split('_')[2]\n        if part=='ps':\n            file_emotion.append('surprise')\n        else:\n            file_emotion.append(part)\n        file_path.append(Tess + dir + '/' + file)\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nTess_df = pd.concat([emotion_df, path_df], axis=1)\nTess_df.head()\nprint(Tess_df.Emotions.value_counts())\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.672Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**SAVEE**","metadata":{}},{"cell_type":"code","source":"savee_directory_list = os.listdir(Savee)\n\nfile_emotion = []\nfile_path = []\n\nfor file in savee_directory_list:\n    file_path.append(Savee + file)\n    part = file.split('_')[1]\n    ele = part[:-6]\n    if ele=='a':\n        file_emotion.append('angry')\n    elif ele=='d':\n        file_emotion.append('disgust')\n    elif ele=='f':\n        file_emotion.append('fear')\n    elif ele=='h':\n        file_emotion.append('happy')\n    elif ele=='n':\n        file_emotion.append('neutral')\n    elif ele=='sa':\n        file_emotion.append('sad')\n    else:\n        file_emotion.append('surprise')\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nSavee_df = pd.concat([emotion_df, path_df], axis=1)\nSavee_df.head()\nprint(Savee_df.Emotions.value_counts())\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# creating Dataframe using all the 4 dataframes we created so far.\ndata_path = pd.concat([ravdess_df,Savee_df,Tess_df,Crema_df], axis = 0)\ndata_path.to_csv(\"data_path.csv\",index=False)\ndata_path.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.672Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":",Savee_df,Tess_df,Crema_df","metadata":{}},{"cell_type":"code","source":"print(data_path.Emotions.value_counts())\ndata_path.shape","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.title('Count of Emotions', size=16)\nsns.countplot(x='Emotions', data=data_path)\nplt.ylabel('Count', size=12)\nplt.xlabel('Emotions', size=12)\nplt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data,sr = librosa.load(file_path[0])\ndata.shape","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mfcc = librosa.feature.mfcc(y=data, sr=sr, n_mfcc=30)\nprint(mfcc.shape)\n# MFCC\nplt.figure(figsize=(16, 10))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(data,rate=sr)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.672Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Augmentation'","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport librosa\nimport scipy.signal\n\n# Augmentation Functions\ndef add_noise(data, noise_factor=0.035):\n    noise_amp = noise_factor * np.random.uniform() * np.amax(data)\n    noisy_data = data + noise_amp * np.random.normal(size=data.shape)\n    return noisy_data\n\ndef stretch(data, rate=0.8):\n    return librosa.effects.time_stretch(y=data, rate=rate)\n\ndef shift(data, max_shift=5):\n    shift_range = int(np.random.uniform(low=-max_shift, high=max_shift) * 1000)\n    return np.roll(data, shift_range)\n\ndef pitch(data, sampling_rate, pitch_factor=0.7):\n    return librosa.effects.pitch_shift(y=data, sr=sampling_rate, n_steps=pitch_factor)\ndef equalize(data, sr, cutoff=3000):\n    # Simple high-shelf filter to boost higher frequencies\n    b, a = scipy.signal.butter(6, cutoff / (0.5 * sr), btype='high', analog=False)\n    return scipy.signal.filtfilt(b, a, data)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.672Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Extraction","metadata":{}},{"cell_type":"markdown","source":"# basic feature extraction","metadata":{}},{"cell_type":"code","source":"# def mfcc(data, sr, frame_length=2048, hop_length=512, flatten: bool = True):\n#     mfcc = librosa.feature.mfcc(y=data, sr=sr, n_mfcc=13, n_fft=frame_length, hop_length=hop_length)\n#     return np.squeeze(mfcc.T) if not flatten else np.ravel(mfcc.T)\n\n# def extract_features(data, sr=22050, frame_length=2048, hop_length=512):\n#     return mfcc(data, sr, frame_length, hop_length)\n\n# # Final get_features function\n# def get_features(path, duration=2.5, offset=0.6):\n#     data, sr = librosa.load(path, duration=duration, offset=offset)\n#     audio = [extract_features(data, sr)]\n\n#     # Apply augmentations\n#     noised = add_noise(data)\n#     audio.append(extract_features(noised, sr))\n\n#     pitched = pitch(data, sr)\n#     audio.append(extract_features(pitched, sr))\n\n#     noised_pitched = add_noise(pitched)\n#     audio.append(extract_features(noised_pitched, sr))\n\n#     equalized = equalize(data, sr)\n#     audio.append(extract_features(equalized, sr))\n\n#     return np.vstack(audio)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.673Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Keeping the 2d structure","metadata":{}},{"cell_type":"code","source":"# import numpy as np\n# import librosa\n\n# def mfcc(data, sr, frame_length=2048, hop_length=512):\n#     \"\"\"\n#     Compute MFCCs and return as a 2D array (time_steps, n_mfcc).\n#     \"\"\"\n#     m = librosa.feature.mfcc(\n#         y=data, sr=sr,\n#         n_mfcc=13,\n#         n_fft=frame_length,\n#         hop_length=hop_length\n#     )\n#     # m has shape (n_mfcc, time_steps) → transpose to (time_steps, n_mfcc)\n#     return m.T\n\n# def extract_features(data, sr=22050, frame_length=2048, hop_length=512):\n#     \"\"\"\n#     Wrapper that just returns the 2D MFCC array.\n#     \"\"\"\n#     return mfcc(data, sr, frame_length, hop_length)\n\n# def get_features(path, duration=2.5, offset=0.6):\n#     \"\"\"\n#     Loads the audio, applies augmentations, and returns\n#     a single NumPy array of shape (num_augs, time_steps, n_mfcc).\n#     \"\"\"\n#     # 1) Load and compute on clean signal\n#     data, sr = librosa.load(path, duration=duration, offset=offset)\n#     feats = [extract_features(data, sr)]\n\n#     # 2) Noise\n#     noised = add_noise(data)\n#     feats.append(extract_features(noised, sr))\n\n#     # 3) Pitch shift\n#     pitched = pitch(data, sr)\n#     feats.append(extract_features(pitched, sr))\n\n#     # 4) Pitch + noise\n#     noised_pitched = add_noise(pitched)\n#     feats.append(extract_features(noised_pitched, sr))\n\n#     # 5) Equalization\n#     equalized = equalize(data, sr)\n#     feats.append(extract_features(equalized, sr))\n\n#     # Stack into shape (5, time_steps, n_mfcc)\n#     return np.stack(feats, axis=0)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.673Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Using DelMFCC and Del2MFCC","metadata":{}},{"cell_type":"code","source":"# import numpy as np\n# import librosa\n\n# def extract_mfcc_features(data, sr, frame_length=2048, hop_length=512, flatten=True):\n#     # Base MFCCs\n#     mfcc = librosa.feature.mfcc(y=data, sr=sr, n_mfcc=13, n_fft=frame_length, hop_length=hop_length)\n    \n#     # First-order derivative (delta)\n#     mfcc_delta = librosa.feature.delta(mfcc)\n    \n#     # Second-order derivative (delta-delta)\n#     mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n    \n#     # Stack: (n_mfcc * 3, time) → Transpose to (time, features)\n#     features = np.vstack([mfcc, mfcc_delta, mfcc_delta2])\n    \n#     if flatten:\n#         return np.ravel(features.T)\n#     else:\n#         return np.squeeze(features.T)\n\n# def extract_features(data, sr=22050, frame_length=2048, hop_length=512):\n#     return extract_mfcc_features(data, sr, frame_length, hop_length)\n\n# def get_features(path, duration=2.5, offset=0.6):\n#     data, sr = librosa.load(path, duration=duration, offset=offset)\n#     audio = [extract_features(data, sr)]\n\n#     # Apply augmentations\n#     audio.append(extract_features(add_noise(data), sr))\n#     audio.append(extract_features(pitch(data, sr), sr))\n#     audio.append(extract_features(add_noise(pitch(data, sr)), sr))\n#     audio.append(extract_features(equalize(data, sr), sr))\n\n#     return np.vstack(audio)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.673Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"keeping the 2d structure","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport librosa\n\ndef mfcc(data, sr, frame_length=2048, hop_length=512):\n    \"\"\"\n    Compute MFCC + delta + delta-delta as a (time_steps, 39) feature matrix.\n    \"\"\"\n    mfccs = librosa.feature.mfcc(\n        y=data, sr=sr,\n        n_mfcc=13,\n        n_fft=frame_length,\n        hop_length=hop_length\n    )  # shape: (13, T)\n\n    delta_mfccs = librosa.feature.delta(mfccs)       # shape: (13, T)\n    delta2_mfccs = librosa.feature.delta(mfccs, order=2)  # shape: (13, T)\n\n    # Stack all features along axis 0 → shape: (39, T)\n    combined = np.vstack([mfccs, delta_mfccs, delta2_mfccs])  # shape: (39, T)\n\n    # Transpose to shape (T, 39)\n    return combined.T\n\ndef extract_features(data, sr=22050, frame_length=2048, hop_length=512):\n    return mfcc(data, sr, frame_length, hop_length)\n\ndef get_features(path, duration=2.5, offset=0.6):\n    data, sr = librosa.load(path, duration=duration, offset=offset)\n    feats = [extract_features(data, sr)]\n\n    # Augmentations\n    noised = add_noise(data)\n    feats.append(extract_features(noised, sr))\n\n    pitched = pitch(data, sr)\n    feats.append(extract_features(pitched, sr))\n\n    noised_pitched = add_noise(pitched)\n    feats.append(extract_features(noised_pitched, sr))\n\n    equalized = equalize(data, sr)\n    feats.append(extract_features(equalized, sr))\n\n    return np.stack(feats, axis=0)  # shape: (5, time_steps, 39)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import multiprocessing as mp\nprint(\"Number of processors: \", mp.cpu_count())","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.673Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Get Features","metadata":{}},{"cell_type":"code","source":"import timeit\nfrom tqdm import tqdm\nstart = timeit.default_timer()\nX,Y=[],[]\nfor path,emotion,index in tqdm (zip(data_path.Path,data_path.Emotions,range(data_path.Path.shape[0]))):\n    features=get_features(path)\n    if index%500==0:\n        print(f'{index} audio has been processed')\n    for i in features:\n        X.append(i)\n        Y.append(emotion)\nprint('Done')\nstop = timeit.default_timer()\n\nprint('Time: ', stop - start)         ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(X), len(Y), data_path.Path.shape","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.673Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Saving Features","metadata":{}},{"cell_type":"code","source":"# Emotions = pd.DataFrame(X)\n# Emotions['Emotions'] = Y\n# Emotions.to_csv('emotion.csv', index=False)\n# Emotions.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.675Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Emotions = pd.read_csv('./emotion.csv')\n# Emotions.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.676Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(Emotions.isna().any())","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.676Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Emotions=Emotions.fillna(0)\n# print(Emotions.isna().any())\n# Emotions.shape","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.676Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# np.sum(Emotions.isna())","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.676Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"code","source":"# #taking all rows and all cols without last col for X which include features\n# #taking last col for Y, which include the emotions\n\n\n# X = Emotions.iloc[: ,:-1].values\n# Y = Emotions['Emotions'].values\n# print(Y)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.676Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # As this is a multiclass classification problem onehotencoding our Y\n# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n# encoder = OneHotEncoder()\n# Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()\n\n# label_order = encoder.categories_[0].tolist()\n# print(label_order)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.676Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# encoder.categories_[0].tolist()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(Y.shape)\n# X.shape","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n\n# x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=42,test_size=0.2, shuffle=True)\n# x_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #reshape for gru\n# X_train = x_train.reshape(x_train.shape[0] , x_train.shape[1] , 1)\n# X_test = x_test.reshape(x_test.shape[0] , x_test.shape[1] , 1)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # scaling our data with sklearn's Standard scaler\n# scaler = StandardScaler()\n# x_train = scaler.fit_transform(x_train)\n# x_test = scaler.transform(x_test)\n# x_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import keras\n# from keras.preprocessing import sequence\n# from keras.models import Sequential\n# from keras.layers import Dense, Embedding\n# from keras.layers import LSTM,BatchNormalization , GRU\n# from tensorflow.keras.utils import to_categorical\n# from keras.layers import Input, Flatten, Dropout, Activation\n# from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n# from keras.models import Model\n# from keras.callbacks import ModelCheckpoint\n# from tensorflow.keras.optimizers import SGD\n# from keras.saving import register_keras_serializable","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #Reshape for CNN_GRU MODEL\n# x_traincnn = np.expand_dims(x_train, axis=-1)\n# x_testcnn = np.expand_dims(x_test, axis=-1)\n# x_traincnn.shape, y_train.shape, x_testcnn.shape, y_test.shape\n# #x_testcnn[0]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.677Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Code","metadata":{}},{"cell_type":"code","source":"# import tensorflow as tf\n# from tensorflow.keras import layers as L\n# from tensorflow.keras.utils import register_keras_serializable\n\n# def res_se_block(x, filters, kernel_size, pool_size):\n#     # --- First conv path ---\n#     y = L.Conv1D(filters, kernel_size, padding='same', activation='relu')(x)\n#     y = L.BatchNormalization()(y)\n\n#     # --- Squeeze & Excitation ---\n#     se = L.GlobalAveragePooling1D()(y)              # (batch, filters)\n#     se = L.Dense(filters // 8, activation='relu')(se)\n#     se = L.Dense(filters, activation='sigmoid')(se) # (batch, filters)\n#     se = L.Reshape((1, filters))(se)                 # (batch, 1, filters)\n#     y  = L.Multiply()([y, se])                      # broadcast to (batch, time, filters)\n\n#     # --- Second conv & skip connection ---\n#     y = L.Conv1D(filters, kernel_size, padding='same')(y)\n#     y = L.BatchNormalization()(y)\n#     if x.shape[-1] != filters:\n#         # project skip to match filters\n#         x = L.Conv1D(filters, 1, padding='same')(x)\n#         x = L.BatchNormalization()(x)\n#     y = L.Add()([x, y])\n#     y = L.Activation('relu')(y)\n\n#     # --- Pooling ---\n#     y = L.MaxPool1D(pool_size, padding='same')(y)\n#     return y\n\n# # --- Lightweight Self-Attention Layer ---\n# @register_keras_serializable()\n# class SimpleAttention(L.Layer):\n#     def __init__(self, **kwargs):\n#         super().__init__(**kwargs)\n#         self.att = L.Attention()\n#     def call(self, x):\n#         return self.att([x, x])\n#     def get_config(self):\n#         return super().get_config()\n\n# # --- Build the model ---\n# inp = L.Input(shape=(1404, 1))\n\n# x = res_se_block(inp, filters=64,  kernel_size=5, pool_size=3)\n# x = res_se_block(x,    filters=128, kernel_size=3, pool_size=3)\n# x = res_se_block(x,    filters=256, kernel_size=3, pool_size=2)\n# x = L.Dropout(0.3)(x)\n\n# x = L.Dense(128, activation='relu')(x)\n# x = SimpleAttention()(x)\n# x = L.Dropout(0.3)(x)\n\n# x = L.GlobalAveragePooling1D()(x)\n# x = L.Dense(64, activation='relu')(x)\n# x = L.Dropout(0.4)(x)\n\n# out = L.Dense(7, activation='softmax')(x)\n\n# model = tf.keras.Model(inp, out)\n# model.compile(\n#     optimizer='adam',\n#     loss='categorical_crossentropy',\n#     metrics=['accuracy']\n# )\n# model.summary()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# history=model.fit(x_traincnn, y_train, epochs=100, validation_data=(x_testcnn, y_test), batch_size=32)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Get number of epochs based on training history\n# epochs = range(len(history.history['loss']))\n\n# fig , ax = plt.subplots(1,2)\n# train_acc = history.history['accuracy']\n# train_loss = history.history['loss']\n# test_acc = history.history['val_accuracy']\n# test_loss = history.history['val_loss']\n\n# fig.set_size_inches(20,6)\n\n# # Plot Loss\n# ax[0].plot(epochs, train_loss, label='Training Loss')\n# ax[0].plot(epochs, test_loss, label='Testing Loss')\n# ax[0].set_title('Training & Testing Loss')\n# ax[0].legend()\n# ax[0].set_xlabel(\"Epochs\")\n\n# # Plot Accuracy\n# ax[1].plot(epochs, train_acc, label='Training Accuracy')\n# ax[1].plot(epochs, test_acc, label='Testing Accuracy')\n# ax[1].set_title('Training & Testing Accuracy')\n# ax[1].legend()\n# ax[1].set_xlabel(\"Epochs\")\n\n# plt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import tensorflow as tf\n# import tensorflow.keras.layers as L\n\n# # ====== Positional Embedding Layer ======\n# @register_keras_serializable()\n# class PositionalEmbedding(L.Layer):\n#     def __init__(self, sequence_length, embed_dim, **kwargs):\n#         super(PositionalEmbedding, self).__init__(**kwargs)\n#         self.sequence_length = sequence_length\n#         self.embed_dim = embed_dim\n#         self.pos_emb = self.add_weight(\n#             name=\"pos_emb\",\n#             shape=(1, sequence_length, embed_dim),\n#             initializer=\"random_normal\",\n#             trainable=True,\n#         )\n\n#     def call(self, x):\n#         return x + self.pos_emb\n\n#     def get_config(self):\n#         config = super(PositionalEmbedding, self).get_config()\n#         config.update({\n#             \"sequence_length\": self.sequence_length,\n#             \"embed_dim\": self.embed_dim\n#         })\n#         return config\n\n# # ====== Transformer Block ======\n# @register_keras_serializable()\n# class TransformerBlock(L.Layer):\n#     def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n#         super(TransformerBlock, self).__init__(**kwargs)\n#         self.embed_dim = embed_dim\n#         self.num_heads = num_heads\n#         self.ff_dim = ff_dim\n#         self.rate = rate\n\n#         self.att = L.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n#         self.ffn = tf.keras.Sequential([\n#             L.Dense(ff_dim, activation=\"relu\"),\n#             L.Dense(embed_dim)\n#         ])\n#         self.layernorm1 = L.LayerNormalization(epsilon=1e-6)\n#         self.layernorm2 = L.LayerNormalization(epsilon=1e-6)\n#         self.dropout1 = L.Dropout(rate)\n#         self.dropout2 = L.Dropout(rate)\n\n#     def call(self, inputs, training=False):\n#         attn_output = self.att(inputs, inputs)\n#         attn_output = self.dropout1(attn_output, training=training)\n#         out1 = self.layernorm1(inputs + attn_output)\n#         ffn_output = self.ffn(out1)\n#         ffn_output = self.dropout2(ffn_output, training=training)\n#         return self.layernorm2(out1 + ffn_output)\n\n#     def get_config(self):\n#         config = super(TransformerBlock, self).get_config()\n#         config.update({\n#             \"embed_dim\": self.embed_dim,\n#             \"num_heads\": self.num_heads,\n#             \"ff_dim\": self.ff_dim,\n#             \"rate\": self.rate\n#         })\n#         return config\n\n# # ====== Build the model ======\n# input_layer = L.Input(shape=(1404, 1))\n\n# # CNN Feature Extractor\n# x = L.Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu')(input_layer)\n# x = L.BatchNormalization()(x)\n# x = L.MaxPool1D(pool_size=3, strides=2, padding='same')(x)\n\n# x = L.Conv1D(128, kernel_size=3, strides=1, padding='same', activation='relu')(x)\n# x = L.BatchNormalization()(x)\n# x = L.MaxPool1D(pool_size=3, strides=2, padding='same')(x)\n\n# # ====== Added new CNN layer here ======\n# x = L.Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu')(x)\n# x = L.BatchNormalization()(x)\n# x = L.MaxPool1D(pool_size=2, strides=2, padding='same')(x)\n\n# x = L.Dropout(0.3)(x)\n\n# # Project to smaller dimension\n# x = L.Dense(64, activation='relu')(x)\n\n# # Add Positional Embedding\n# sequence_length = x.shape[1]\n# x = PositionalEmbedding(sequence_length, 64)(x)\n\n# # Transformer Blocks\n# x = TransformerBlock(embed_dim=64, num_heads=4, ff_dim=256)(x)\n# x = TransformerBlock(embed_dim=64, num_heads=4, ff_dim=256)(x)\n# x = L.Dropout(0.3)(x)\n\n# # BiGRU Layers\n# x = L.Bidirectional(L.GRU(256, return_sequences=True, dropout=0.3))(x)\n# x = L.Bidirectional(L.GRU(128, dropout=0.3))(x)\n\n# # Dense Layers\n# x = L.Dense(128, activation='relu')(x)\n# x = L.BatchNormalization()(x)\n# x = L.Dropout(0.4)(x)\n\n# # Output Layer\n# output_layer = L.Dense(7, activation='softmax')(x)\n\n# # Define Model\n# model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n\n# # ====== Compile the model ======\n# loss_fn = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)  # Label smoothing\n# model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n\n# # ====== Summary ======\n# model.summary()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# history=model.fit(x_traincnn, y_train, epochs=100, validation_data=(x_testcnn, y_test), batch_size=32)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Get number of epochs based on training history\n# epochs = range(len(history.history['loss']))\n\n# fig , ax = plt.subplots(1,2)\n# train_acc = history.history['accuracy']\n# train_loss = history.history['loss']\n# test_acc = history.history['val_accuracy']\n# test_loss = history.history['val_loss']\n\n# fig.set_size_inches(20,6)\n\n# # Plot Loss\n# ax[0].plot(epochs, train_loss, label='Training Loss')\n# ax[0].plot(epochs, test_loss, label='Testing Loss')\n# ax[0].set_title('Training & Testing Loss')\n# ax[0].legend()\n# ax[0].set_xlabel(\"Epochs\")\n\n# # Plot Accuracy\n# ax[1].plot(epochs, train_acc, label='Training Accuracy')\n# ax[1].plot(epochs, test_acc, label='Testing Accuracy')\n# ax[1].set_title('Training & Testing Accuracy')\n# ax[1].legend()\n# ax[1].set_xlabel(\"Epochs\")\n\n# plt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.678Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Combining both the models","metadata":{}},{"cell_type":"markdown","source":"data preparation","metadata":{}},{"cell_type":"code","source":"#encode labels and split dataset\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nlabel_enc = LabelEncoder()\nY_encoded = label_enc.fit_transform(Y)  # convert emotion labels to integers\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, Y_encoded, test_size=0.2, stratify=Y_encoded, random_state=42\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#create padded tensorflow dataset\nimport tensorflow as tf\n\nn_mfcc = X_train[0].shape[1]  # typically 13\n\ndef make_dataset(X, Y, batch_size=32, shuffle=True):\n    def gen():\n        for x, y in zip(X, Y):\n            yield x.astype(np.float32), y\n    ds = tf.data.Dataset.from_generator(\n        gen,\n        output_signature=(\n            tf.TensorSpec(shape=(None, n_mfcc), dtype=tf.float32),\n            tf.TensorSpec(shape=(), dtype=tf.int32),\n        )\n    )\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(X))\n    ds = ds.padded_batch(batch_size, padded_shapes=([None, n_mfcc], []))\n    return ds.prefetch(tf.data.AUTOTUNE)\n\ntrain_ds = make_dataset(X_train, y_train, batch_size=32, shuffle=True)\nval_ds   = make_dataset(X_test,  y_test,  batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import tensorflow as tf\n# from tensorflow.keras import layers as L\n\n# def res_se_block(x, filters, kernel_size, pool_size):\n#     shortcut = x\n#     y = L.Conv1D(filters, kernel_size, padding='same', activation='relu')(x)\n#     y = L.BatchNormalization()(y)\n\n#     # Squeeze and Excitation\n#     se = L.GlobalAveragePooling1D()(y)\n#     se = L.Dense(filters // 8, activation='relu')(se)\n#     se = L.Dense(filters, activation='sigmoid')(se)\n#     se = L.Reshape((1, filters))(se)\n#     y = L.Multiply()([y, se])\n\n#     y = L.Conv1D(filters, kernel_size, padding='same')(y)\n#     y = L.BatchNormalization()(y)\n\n#     if shortcut.shape[-1] != filters:\n#         shortcut = L.Conv1D(filters, 1, padding='same')(shortcut)\n#         shortcut = L.BatchNormalization()(shortcut)\n\n#     y = L.Add()([shortcut, y])\n#     y = L.Activation('relu')(y)\n#     return L.MaxPool1D(pool_size, padding='same')(y)\n\n# class PositionalEmbedding(L.Layer):\n#     def __init__(self, embed_dim, maxlen=500, **kw):\n#         super().__init__(**kw)\n#         self.embed_dim = embed_dim\n#         self.maxlen = maxlen\n\n#     def build(self, inp_shape):\n#         self.pos_emb = self.add_weight(\n#             shape=(1, self.maxlen, self.embed_dim),\n#             initializer=\"random_normal\",\n#             trainable=True\n#         )\n\n#     def call(self, x):\n#         length = tf.shape(x)[1]\n#         return x + self.pos_emb[:, :length, :]\n\n# class TransformerBlock(L.Layer):\n#     def __init__(self, emb_dim, heads, ff_dim, rate=0.1, **kw):\n#         super().__init__(**kw)\n#         self.att = L.MultiHeadAttention(heads, key_dim=emb_dim)\n#         self.ff = tf.keras.Sequential([\n#             L.Dense(ff_dim, activation='relu'),\n#             L.Dense(emb_dim),\n#         ])\n#         self.norm1 = L.LayerNormalization(epsilon=1e-6)\n#         self.norm2 = L.LayerNormalization(epsilon=1e-6)\n#         self.drop1 = L.Dropout(rate)\n#         self.drop2 = L.Dropout(rate)\n\n#     def call(self, x, training=False):\n#         attn_output = self.att(x, x)\n#         x = self.norm1(x + self.drop1(attn_output, training=training))\n#         ff_output = self.ff(x)\n#         return self.norm2(x + self.drop2(ff_output, training=training))\n\n# # Input layer (MFCC + delta + delta-delta)\n# inp = L.Input(shape=(None, 39))  # (time_steps, 39)\n\n# # SE-ResNet blocks\n# x = res_se_block(inp, 64, 5, 2)\n# x = res_se_block(x, 128, 3, 2)\n# x = res_se_block(x, 256, 3, 2)\n# x = L.Dropout(0.3)(x)\n\n# # Channel reduction + Transformer encoder\n# x = L.Dense(128, activation='relu')(x)\n# x = PositionalEmbedding(embed_dim=128, maxlen=500)(x)\n# x = TransformerBlock(128, heads=4, ff_dim=256)(x)\n# x = TransformerBlock(128, heads=4, ff_dim=256)(x)\n\n# # Bidirectional GRU\n# x = L.Bidirectional(L.GRU(128, return_sequences=True, dropout=0.3))(x)\n# x = L.Bidirectional(L.GRU(64, dropout=0.3))(x)\n\n# # Dense + regularization\n# x = L.Dense(128, activation='relu')(x)\n# x = L.BatchNormalization()(x)\n# x = L.Dropout(0.5)(x)\n\n# # Output\n# out = L.Dense(7, activation='softmax')(x)\n\n# # Build and compile model\n# model = tf.keras.Model(inp, out)\n# model.compile(\n#     optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n#     loss='sparse_categorical_crossentropy',\n#     metrics=['accuracy']\n# )\n# model.summary()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.678Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"TCN Layer","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers as L\n\n# Residual SE Block\ndef res_se_block(x, filters, kernel_size, pool_size):\n    y = L.Conv1D(filters, kernel_size, padding='same', activation='relu')(x)\n    y = L.BatchNormalization()(y)\n    se = L.GlobalAveragePooling1D()(y)\n    se = L.Dense(filters // 8, activation='relu')(se)\n    se = L.Dense(filters, activation='sigmoid')(se)\n    se = L.Reshape((1, filters))(se)\n    y = L.Multiply()([y, se])\n    y = L.Conv1D(filters, kernel_size, padding='same')(y)\n    y = L.BatchNormalization()(y)\n    if x.shape[-1] != filters:\n        x = L.Conv1D(filters, 1, padding='same')(x)\n        x = L.BatchNormalization()(x)\n    y = L.Add()([x, y])\n    y = L.Activation('relu')(y)\n    return L.MaxPool1D(pool_size, padding='same')(y)\n\n# Positional Embedding\nclass PositionalEmbedding(L.Layer):\n    def __init__(self, embed_dim, maxlen=500, **kw):\n        super().__init__(**kw)\n        self.embed_dim = embed_dim\n        self.maxlen = maxlen\n    def build(self, inp_shape):\n        self.pos_emb = self.add_weight(\n            shape=(1, self.maxlen, self.embed_dim),\n            initializer=\"random_normal\", trainable=True)\n    def call(self, x):\n        length = tf.shape(x)[1]\n        return x + self.pos_emb[:, :length, :]\n\n# TCN block\ndef tcn_block(x, filters, kernel_size, dilation_rate, dropout_rate):\n    shortcut = x\n    x = L.Conv1D(filters, kernel_size, padding='causal',\n                 dilation_rate=dilation_rate, activation='relu')(x)\n    x = L.BatchNormalization()(x)\n    x = L.SpatialDropout1D(dropout_rate)(x)\n    x = L.Conv1D(filters, kernel_size, padding='causal',\n                 dilation_rate=dilation_rate, activation='relu')(x)\n    x = L.BatchNormalization()(x)\n    if shortcut.shape[-1] != filters:\n        shortcut = L.Conv1D(filters, 1, padding='same')(shortcut)\n    x = L.Add()([shortcut, x])\n    return L.Activation('relu')(x)\n\n# Model\ninp = L.Input(shape=(None, 39))  # MFCC + delta + delta-delta = 39\n\n# SE-ResNet Frontend\nx = res_se_block(inp, 64, 5, 3)\nx = res_se_block(x, 128, 3, 3)\nx = res_se_block(x, 256, 3, 2)\nx = L.Dropout(0.3)(x)\n\n# Dense + TCN block\nx = L.Dense(128, activation='relu')(x)\nx = PositionalEmbedding(embed_dim=128)(x)\nx = tcn_block(x, filters=128, kernel_size=3, dilation_rate=2, dropout_rate=0.2)\nx = tcn_block(x, filters=128, kernel_size=3, dilation_rate=4, dropout_rate=0.2)\n\n# BiGRU\nx = L.Bidirectional(L.GRU(128, return_sequences=True, dropout=0.3))(x)\nx = L.Bidirectional(L.GRU(64, dropout=0.3))(x)\n\n# Classifier head\nx = L.Dense(128, activation='relu')(x)\nx = L.Dropout(0.4)(x)\nout = L.Dense(7, activation='softmax')(x)\n\nmodel = tf.keras.Model(inp, out)\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history=model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=100,\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the full model (architecture + weights + optimizer) in the modern .keras format\nmodel.save(\"CNN_model1.keras\")\nprint(\"✅ Model saved to disk as CNN_model1.keras\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get number of epochs based on training history\nepochs = range(len(history.history['loss']))\n\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_accuracy']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\n\n# Plot Loss\nax[0].plot(epochs, train_loss, label='Training Loss')\nax[0].plot(epochs, test_loss, label='Testing Loss')\nax[0].set_title('Training & Testing Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\n# Plot Accuracy\nax[1].plot(epochs, train_acc, label='Training Accuracy')\nax[1].plot(epochs, test_acc, label='Testing Accuracy')\nax[1].set_title('Training & Testing Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.679Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Validation Accuracy","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 6))\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training vs Validation Accuracy')\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.679Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Printing the confusion matrix","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Get predictions and true labels\ny_true = []\ny_pred = []\n\nfor x_batch, y_batch in val_ds:\n    preds = model.predict(x_batch)\n    y_true.extend(y_batch.numpy())\n    y_pred.extend(np.argmax(preds, axis=1))\n\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define class labels in the correct order\nemotion_labels = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n\ncm = confusion_matrix(y_true, y_pred)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', xticklabels=emotion_labels, yticklabels=emotion_labels, cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(classification_report(y_true, y_pred, target_names=emotion_labels))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-07T08:05:54.679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}